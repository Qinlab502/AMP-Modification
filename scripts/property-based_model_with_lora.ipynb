{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5359fb23-066e-4a83-8a4a-593d254aa339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import EsmTokenizer, EsmForSequenceClassification, EsmModel, EsmConfig\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from focal_loss.focal_loss import FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d265097-765c-4e1c-bb53-4293d0cceaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set PyTorch to use GPU or CPU accordingly\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df715623-2699-4845-93ad-72384b4dd142",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 24\n",
    "lr = 1e-3\n",
    "num_epochs = 20\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e999fed-d408-4df7-8dbb-1904fccd531e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/esm2_650M were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at ../models/esm2_650M and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"../models/esm2_650M\"\n",
    "tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "model = EsmForSequenceClassification.from_pretrained(model_name, num_labels=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49e2f4-6882-44bc-bd0c-0e7080620ad5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get the physicochemical property labels\n",
    "    positive label: net charge∈[2,7]; hydrophobicity∈[0.5,0.7]  \n",
    "    negative label: out of that range  \n",
    "<font size=3>The range has been learned through our recent work: https://doi.org/10.1101/2022.11.03.515123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d59795-0185-4c95-82ed-8fc62740952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_code(pop):\n",
    "    peptide=[]\n",
    "    for acid in pop:\n",
    "        peptide.append(dic_new[acid])\n",
    "    \n",
    "    return np.array(peptide)\n",
    "    \n",
    "def get_mean_hydrophobicity(pop): #hydrophobicity value calculations\n",
    "    pop=translate_to_code(pop)\n",
    "    \n",
    "    hyd=[]\n",
    "    for i in range(len(pop)):\n",
    "        hyd.append(hydrophobicity[pop[i]])\n",
    "    hyd=np.array(hyd)\n",
    "    mean_hydrophobicity=sum(hyd)/(len(pop))\n",
    "    return mean_hydrophobicity\n",
    "\n",
    "def get_net_Charge(pop):# Net-charge calculations\n",
    "    pop=translate_to_code(pop)\n",
    "    \n",
    "    elc=0\n",
    "    for i in pop:\n",
    "        i=int(i)\n",
    "        if i==10 or i==15 or i==21:\n",
    "            elc+=1\n",
    "        elif i==9 or i==13:\n",
    "            elc-=1\n",
    "        else: pass \n",
    "    return np.array(elc,'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac53035e-c8a1-49a5-a23b-bf326daab99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acid = {\n",
    "    4: 'L', \n",
    "    5: 'A', \n",
    "    6: 'G', \n",
    "    7: 'V', \n",
    "    8: 'S', \n",
    "    9: 'E', \n",
    "    10: 'R', \n",
    "    11: 'T', \n",
    "    12: 'I', \n",
    "    13: 'D',\n",
    "    14: 'P', \n",
    "    15: 'K', \n",
    "    16: 'Q', \n",
    "    17: 'N', \n",
    "    18: 'F', \n",
    "    19: 'Y', \n",
    "    20: 'M', \n",
    "    21: 'H', \n",
    "    22: 'W', \n",
    "    23: 'C'}\n",
    "hydrophobicity={\n",
    "    4: 1.700,\n",
    "    5: 0.310,\n",
    "    6: 0.,\n",
    "    7: 1.220,\n",
    "    8: -0.040,\n",
    "    9: -0.640,\n",
    "    10: -1.010,\n",
    "    11: 0.260,\n",
    "    12: 1.800,\n",
    "    13: -0.770,\n",
    "    14: 0.720,\n",
    "    15: -0.990,\n",
    "    16: -0.220,\n",
    "    17: -0.600,\n",
    "    18: 1.790,\n",
    "    19: 0.960,\n",
    "    20: 1.230,\n",
    "    21: 0.130,\n",
    "    22: 2.250,\n",
    "    23: 1.540\n",
    "}\n",
    "dic_new = dict(zip(amino_acid.values(), amino_acid.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d1d55f-a133-4bf3-91bb-e65d06655533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasta_dict(fasta_file):\n",
    "    fasta_dict = {}\n",
    "    with open(fasta_file, 'r') as infile:\n",
    "        for line in infile:\n",
    "            if line.startswith(\">\"):\n",
    "                head = line.replace(\"\\n\", \"\").replace(\">\", \"\")\n",
    "                fasta_dict[head] = ''\n",
    "            else:\n",
    "                fasta_dict[head] += line.replace(\"\\n\", \"\")\n",
    "    return fasta_dict\n",
    "\n",
    "def get_label(fasta_dict):\n",
    "    label=[]\n",
    "    for header,sequence in fasta_dict.items():\n",
    "        charge=get_net_Charge(sequence)\n",
    "        hy=get_mean_hydrophobicity(sequence)\n",
    "        if 2.0 <= charge <= 7.0 and 0.5 <= hy <= 0.7:\n",
    "            label.append(0)\n",
    "        else:\n",
    "            label.append(1)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab054d7-f00c-4d33-84e0-bc3ef6ef8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, fasta_dict) -> None:\n",
    "        super().__init__()\n",
    "        self.fasta_dict = fasta_dict\n",
    "        self.names = [name for name in fasta_dict.keys()]\n",
    "        self.labels = get_label(fasta_dict)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        seq_name = self.names[idx]\n",
    "        selected_seq = self.fasta_dict[seq_name]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return selected_seq, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97355e84-02c6-459a-b322-27ac436b1671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset checking:\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.030041933059692383,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 135,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e21085c3938493d929dc4a678a1ed9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fasta_dict = get_fasta_dict('../database/LBD_135.fasta')\n",
    "dataset = SeqDataset(fasta_dict)\n",
    "print(\"Dataset checking:\")\n",
    "for i in tqdm(range(dataset.__len__())):\n",
    "    seqs, label = dataset.__getitem__(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f2c07a0-fb97-4c44-9729-5d0695499f50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 71, 1: 64})\n"
     ]
    }
   ],
   "source": [
    "label=get_label(fasta_dict)\n",
    "from collections import Counter\n",
    "c1 = Counter(label)\n",
    "print(c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316e818-7be6-4b35-ac53-b76b6d6e90e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Finetune the ESM2_t33_650M model with Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01f85a40-4514-4572-b2cb-509a28c30a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb17f2dc-f079-406a-a9c4-436cd6ff5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "batch_size = 4\n",
    "dataset_size = len(dataset)\n",
    "train_sample_num = int(dataset_size * 0.8)\n",
    "\n",
    "# Create a list of indices for the dataset\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "# Shuffle the indices using the random seed\n",
    "np.random.seed(random_seed)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split the indices into training and test sets using SubsetRandomSampler\n",
    "train_indices = indices[:train_sample_num]\n",
    "test_indices = indices[train_sample_num:]\n",
    "\n",
    "train_dataset=[]\n",
    "test_dataset=[]\n",
    "for i in train_indices:\n",
    "    train_dataset.append(dataset[i])\n",
    "for i in test_indices:\n",
    "    test_dataset.append(dataset[i])\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e31f5d7d-9064-4e9d-899a-f2dd92f31dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15449604 || all params: 666163865 || trainable%: 2.319189738698901\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03997993469238281,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 540,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a968448e3d184c168e36972adb4f7da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set config file\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, \n",
    "    inference_mode=False, r=48, \n",
    "    lora_alpha=24,\n",
    "    lora_dropout=0.6, \n",
    "    bias=\"none\", \n",
    "    target_modules=[\"query\",\"value\",\"key\"], \n",
    "    modules_to_save=[\"decode_head\"], \n",
    "    # Ensure classification heads are set here to save so we do train them\n",
    ")\n",
    "\n",
    "# Initialize LoRA model\n",
    "lora_model = get_peft_model(model, peft_config)\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(lora_model.parameters(), lr=lr,weight_decay=5e-3)\n",
    "\n",
    "# Calculate number of training steps\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", \n",
    "    optimizer=optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_training_steps         #adjust the learning rate\n",
    ")\n",
    "\n",
    "# Initialize progress bar\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# Replacing the standard cross-entropy loss by a focal loss improved our results slightly.\n",
    "# The gamma parameter is used to adjust the rate at which easy examples are down-weighted. When gamma is zero, Focal Loss is equivalent to Cross Entropy Loss.\n",
    "# As gamma is increased, the effect of the modulating factor is likewise increased, i.e., the loss for well-classified examples (low loss) is down-weighted.\n",
    "focal_loss = FocalLoss(gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef894c0-f53a-4520-9e96-a717bb5bec16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, training_loss: 0.26, test_loss:0.48,training accuracy: 0.5833, test accuracy: 0.5259 \n",
      "Epoch: 1, training_loss: 0.37, test_loss:0.21,training accuracy: 0.5648, test accuracy: 0.6222 \n",
      "Epoch: 2, training_loss: 0.26, test_loss:0.01,training accuracy: 0.7500, test accuracy: 0.8148 \n",
      "Epoch: 3, training_loss: 0.07, test_loss:0.01,training accuracy: 0.8981, test accuracy: 0.8963 \n",
      "Epoch: 4, training_loss: 0.10, test_loss:0.01,training accuracy: 0.8704, test accuracy: 0.9333 \n",
      "Epoch: 5, training_loss: 0.02, test_loss:0.00,training accuracy: 0.9815, test accuracy: 0.9630 \n",
      "Epoch: 6, training_loss: 0.00, test_loss:0.00,training accuracy: 0.9815, test accuracy: 0.9185 \n"
     ]
    }
   ],
   "source": [
    "# Define lists to store the training and validation accuracy for each fold\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "train_loss_list=[]\n",
    "test_loss_list=[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    # Train the model on the training set\n",
    "    lora_model.train()\n",
    "    \n",
    "    for i, (s, l) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = tokenizer(s, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=max_length).to(device)\n",
    "        l = l.to(device)\n",
    "        outputs = lora_model(**inputs, labels = l)\n",
    "        logits = outputs.logits\n",
    "        loss = focal_loss(torch.softmax(logits,dim=1), l.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        #calculate training accuracy\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        total += len(l)\n",
    "        correct += (predictions == l).sum().item()\n",
    "    train_acc=correct / total\n",
    "        \n",
    "    train_loss_list.append(loss.item())\n",
    "    \n",
    "    test_acc = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    lora_model.eval()\n",
    "   \n",
    "    for i, (s, l) in enumerate(test_loader):\n",
    "        inputs = tokenizer(s, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=max_length).to(device)\n",
    "        l = l.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = lora_model(**inputs, labels=l)\n",
    "            logits = outputs.logits\n",
    "            test_loss = focal_loss(torch.softmax(logits,dim=1), l.to(device))\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            total += len(l)\n",
    "            correct += (predictions == l).sum().item()\n",
    "            test_acc=correct / total\n",
    "    \n",
    "    test_loss_list.append(test_loss.item())    \n",
    "\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "    print(\"Epoch: {}, training_loss: {:.2f}, test_loss:{:.2f},training accuracy: {:.4f}, test accuracy: {:.4f} \".format(epoch, loss.item(), test_loss.item(), train_acc, test_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d118bd-0d2f-442a-b339-e3b74cb3737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_model_id = f\"{model_name}_{peft_config.peft_type}_{peft_config.task_type}_{test_acc:.2f}\"\n",
    "# lora_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e30c02-56c0-4281-bf95-1ed888c3f393",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92edf4-eaf8-4467-a89c-b0336d66f980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "config = PeftConfig.from_pretrained('../models/esm2_650M_LORA_SEQ_CLS_0.99')\n",
    "model = EsmForSequenceClassification.from_pretrained('../models/esm2_650M', num_labels=num_classes)\n",
    " \n",
    "model = PeftModel.from_pretrained(model, '../models/esm2_650M_LORA_SEQ_CLS_0.99')\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454503bd-3ff8-48c2-95db-08861024cb00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dict = get_fasta_dict('../database/LBD_test.fasta')\n",
    "dataset = SeqDataset(test_dict)\n",
    "print(\"Dataset checking:\")\n",
    "seqs=[]\n",
    "labels=[]\n",
    "for i in tqdm(range(dataset.__len__())):\n",
    "    seq, label = dataset.__getitem__(i)\n",
    "    seqs.append(seq)\n",
    "    labels.append(label)\n",
    "\n",
    "tokens = tokenizer(seqs, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=max_length).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    predictions=predictions.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009527f-fc78-4c67-af29-0a0a11b055f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "confusion_mat = confusion_matrix(labels, predictions)\n",
    "report = classification_report(predictions, labels)\n",
    "print(f\"Confusion matrix: \\n{confusion_mat}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca768828-18d1-45cb-9d10-c52b697042a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2D UMAP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e30b4-2924-499e-adbb-f6cfb91d9565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185ef16-94bc-4ff8-b4fb-b7cadd10f1c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dict = get_fasta_dict('../database/LBD.fasta')\n",
    "dataset = SeqDataset(test_dict)\n",
    "print(\"Dataset checking:\")\n",
    "seqs=[]\n",
    "pp_label=[]\n",
    "for i in tqdm(range(dataset.__len__())):\n",
    "    seq, label = dataset.__getitem__(i)\n",
    "    seqs.append(seq)\n",
    "    pp_label.append(label)\n",
    "\n",
    "inputs = tokenizer(seqs, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=max_length).to(device)\n",
    "outputs=model.esm(**inputs,output_attentions=True,output_hidden_states=True)\n",
    "test_lora_out=outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263de21-e6c3-4a36-9f08-2a80ac23eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings = UMAP(n_neighbors= 15, \n",
    "                             n_components= 3, \n",
    "                             metric='cosine', spread=1, local_connectivity=2, n_epochs=1000,\n",
    "                             min_dist=1,\n",
    "                             random_state= 420).fit_transform(test_lora_out.mean(1).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b661f9-ed45-43b7-8c24-9396bc076220",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_label=[]\n",
    "for i in range(len(test_dict.keys())):\n",
    "    ac_label.append(list(test_dict.keys())[i].split('|')[-1])\n",
    "\n",
    "df=pd.DataFrame(umap_embeddings,columns=[\"component_0\",\"component_1\",\"component_2\"],index=ac_label)\n",
    "df[\"property\"]=pp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e55f5e8-fcf7-4572-bb44-801ea985c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#DAA520' if i==\"positive\" else '#4682B4' for i in ac_label]\n",
    "shapes = [\"o\" if i==0 else \"x\" for i in pp_label]\n",
    "plt.figure(figsize=[4,3])\n",
    "\n",
    "for i in range(len(shapes)):\n",
    "    plt.scatter(df.iloc[:,0][i], df.iloc[:,1][i], marker=shapes[i], color=colors[i], alpha=1,s=20)\n",
    "    \n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('umap_1')\n",
    "plt.ylabel('umap_2')\n",
    "# plt.savefig('./outputs/umap2d_lora_esm2.png',dpi=1000,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab737370-3bb4-4188-bdd8-61e7572e6abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMP (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
