{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f60c89-690c-47b5-8af0-e1ba34cae9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional, Dict, NamedTuple, Union, Callable\n",
    "import itertools\n",
    "import os\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import squareform, pdist, cdist\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from Bio import SeqIO\n",
    "import biotite.structure as bs\n",
    "from biotite.structure.io.pdbx import PDBxFile, get_structure\n",
    "from biotite.database import rcsb\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import esm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4156b2ee-ff33-478f-b7da-f52a356ce860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import EsmTokenizer, EsmForSequenceClassification, EsmModel, EsmConfig,EsmForMaskedLM\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32496cc7-faac-4a9e-a832-a70a092510d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it in cuda or cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34a4296f-9ffd-4eaa-9490-c705707a0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrize(x):\n",
    "    \"Make layer symmetric in final two dimensions, used for contact prediction.\"\n",
    "    return x + x.transpose(-1, -2)\n",
    "\n",
    "def apc(x):\n",
    "    \"Perform average product correct, used for contact prediction.\"\n",
    "    a1 = x.sum(-1, keepdims=True)\n",
    "    a2 = x.sum(-2, keepdims=True)\n",
    "    a12 = x.sum((-1, -2), keepdims=True)\n",
    "\n",
    "    avg = a1 * a2\n",
    "    avg.div_(a12)  # in-place to reduce memory\n",
    "    normalized = x - avg\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a9a20d-d8fa-4e23-b209-1639dd31c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from https://github.com/facebookresearch/esm/blob/main/esm/modules.py\n",
    "class AttentionLogisticRegression(nn.Module):\n",
    "    \"\"\"Performs symmetrization, apc, and computes a logistic regression on the output features\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features:int,\n",
    "        prepend_bos: bool,\n",
    "        append_eos: bool,\n",
    "        bias=True,\n",
    "        eos_idx: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features=in_features\n",
    "        self.prepend_bos = prepend_bos\n",
    "        self.append_eos = append_eos\n",
    "        if append_eos and eos_idx is None:\n",
    "            raise ValueError(\"Using an alphabet with eos token, but no eos token was passed in.\")\n",
    "        self.eos_idx = eos_idx\n",
    "        self.regression = nn.Linear(in_features, 1, bias)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, attentions):\n",
    "        # remove eos token attentions\n",
    "        if self.append_eos:\n",
    "            eos_mask = tokens.ne(self.eos_idx).to(attentions)\n",
    "            eos_mask = eos_mask.unsqueeze(1) * eos_mask.unsqueeze(2)\n",
    "            attentions = attentions * eos_mask[:, None, None, :, :]\n",
    "            attentions = attentions[..., :-1, :-1]\n",
    "        # remove cls token attentions\n",
    "        if self.prepend_bos:\n",
    "            attentions = attentions[..., 1:, 1:]\n",
    "        batch_size, layers, heads, seqlen, _ = attentions.size()\n",
    "        attentions = attentions.view(batch_size, layers * heads, seqlen, seqlen)\n",
    "        attentions = attentions.to(self.regression.weight.device)  # attentions always float32, may need to convert to float16\n",
    "        attentions= apc(symmetrize(attentions))\n",
    "        attentions = attentions.permute(0, 2, 3, 1)\n",
    "        \n",
    "        return self.activation(self.regression(attentions).squeeze(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86953e8-b30c-4f76-aa3e-9ad2ac43c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasta_dict(fasta_file):\n",
    "    fasta_dict = {}\n",
    "    with open(fasta_file, 'r') as infile:\n",
    "        for line in infile:\n",
    "            if line.startswith(\">\"):\n",
    "                head = line.replace(\"\\n\", \"\").replace(\">\", \"\")\n",
    "                fasta_dict[head] = ''\n",
    "            else:\n",
    "                fasta_dict[head] += line.replace(\"\\n\", \"\")\n",
    "    return fasta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac38cfbf-4941-4a87-8a47-363eebc5f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_contact(attention):\n",
    "    attention=torch.where(attention < 0.9, torch.tensor(0), torch.tensor(1))\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eca8ccbc-7dc4-4462-99eb-5d2cf2edc88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionLogisticRegression(\n",
       "  (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel=torch.load('../models/contact-based model.pt')\n",
    "mymodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5de583bd-f0d5-48ed-b93d-ad5cb55f786e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"../models/esm2_650M\"\n",
    "num_classes=2\n",
    "max_length=24\n",
    "tokenizer = EsmTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2df473a-baf7-499c-bbb9-3f127d9b5826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/esm2_650M were not used when initializing EsmForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at ../models/esm2_650M and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): EsmForSequenceClassification(\n",
       "      (esm): EsmModel(\n",
       "        (embeddings): EsmEmbeddings(\n",
       "          (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (position_embeddings): Embedding(1026, 1280, padding_idx=1)\n",
       "        )\n",
       "        (encoder): EsmEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-32): 33 x EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1280, out_features=1280, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.6, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=48, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=48, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(\n",
       "                    in_features=1280, out_features=1280, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.6, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=48, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=48, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (value): Linear(\n",
       "                    in_features=1280, out_features=1280, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.6, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=48, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=48, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): EsmIntermediate(\n",
       "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (contact_head): EsmContactPredictionHead(\n",
       "          (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): EsmClassificationHead(\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out_proj): Linear(in_features=1280, out_features=2, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): EsmClassificationHead(\n",
       "            (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (out_proj): Linear(in_features=1280, out_features=2, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "config = PeftConfig.from_pretrained('../models/esm2_650M_LORA_SEQ_CLS_0.99')\n",
    "model = EsmForSequenceClassification.from_pretrained('../models/esm2_650M', num_labels=2)\n",
    " \n",
    "model = PeftModel.from_pretrained(model, '../models/esm2_650M_LORA_SEQ_CLS_0.99')\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37d2d1d8-4821-49c5-b64c-20b76945e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbdin_dict=get_fasta_dict('../database/LBD.fasta')\n",
    "seqs=[]\n",
    "for header,seq in lbdin_dict.items():\n",
    "    seqs.append(seq)\n",
    "\n",
    "tokens = tokenizer(seqs, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=max_length).to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    predictions=predictions.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7c8ccf6-fc73-44a3-a8d3-eaccc304214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(seqs, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=max_length).to(device)\n",
    "outputs=model.esm(**inputs,output_attentions=True,output_hidden_states=True)\n",
    "lbdin_lora_attention=torch.stack(outputs.attentions,1)\n",
    "lbdin_outputs=mymodel(inputs['input_ids'],lbdin_lora_attention).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66f5647e-db2d-4a00-b43e-9d0987b305b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=torch.where(predictions==0)\n",
    "lbdin_filter=lbdin_outputs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bec976e-b5d5-4b64-8201-b54d1170f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbdsy_dict=get_fasta_dict('../database/LBD_test.fasta')\n",
    "seqs=[]\n",
    "for header,seq in lbdsy_dict.items():\n",
    "    seqs.append(seq)\n",
    "\n",
    "inputs = tokenizer(seqs, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=max_length).to(device)\n",
    "outputs=model.esm(**inputs,output_attentions=True,output_hidden_states=True)\n",
    "lbdsy_lora_attention=torch.stack(outputs.attentions,1)\n",
    "lbdsy_outputs=mymodel(inputs['input_ids'],lbdsy_lora_attention).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd5a27-398b-4c3c-8e09-4ca98594fa81",
   "metadata": {},
   "source": [
    "## Plan A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec3c9347-c135-4688-ac2d-acadc3e2308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_matrix_intersection(matrices):\n",
    "    # 将第一个矩阵作为基准\n",
    "    intersection_matrix = np.array(matrices[0])\n",
    "\n",
    "    # 逐个矩阵进行逻辑与运算\n",
    "    for matrix in matrices[1:]:\n",
    "        intersection_matrix = np.logical_and(intersection_matrix, matrix)\n",
    "\n",
    "    # 将布尔型矩阵转换为0和1组成的矩阵\n",
    "    intersection_matrix = intersection_matrix.astype(int)\n",
    "\n",
    "    return intersection_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f758166-eb2c-4b97-b0c3-13f172eda38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbdin_pred_contact=get_pred_contact(lbdin_filter)\n",
    "lbdsy_pred_contact=get_pred_contact(lbdsy_outputs)\n",
    "lbd_p=torch.cat([lbdin_pred_contact[:8],lbdsy_pred_contact[:6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8419cd57-f37a-4680-ba98-f58cffa658da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_map=binary_matrix_intersection(np.array(lbd_p))\n",
    "ne_map=binary_matrix_intersection(np.array(lbdin_pred_contact[8:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbc7761d-2676-4b80-89d6-ad07b4ccc955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 8,  9, 13, 15, 16, 16, 18, 19, 20, 21]),\n",
       " array([ 8,  9, 16, 15, 13, 18, 16, 19, 21, 20]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(ac_map != ne_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e914aec9-47b3-4184-8bb8-f5a23ec76a17",
   "metadata": {},
   "source": [
    "## Plan B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4c937e8-7bf7-4463-8549-d2d542cb29c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContactMapRegression(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features:int,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features=in_features\n",
    "        self.regression = nn.Linear(in_features, 2, bias)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,contact_map):\n",
    "        contact_map=contact_map.reshape((contact_map.shape[0],contact_map.shape[1]*contact_map.shape[1]))\n",
    "        outputs=self.activation(self.regression(contact_map))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdb7d10e-4931-47a3-8672-1fe8f8f3b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "contactmodel=ContactMapRegression(22*22)\n",
    "contactmodel.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(contactmodel.parameters(), lr=1e-2)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500  # Adjust the number of epochs as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64c537c7-c468-49bd-829b-0bc712681a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_255787/312588753.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cmin=torch.tensor(cmin,dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "lbd_p=torch.cat([lbdin_pred_contact[:8],lbdsy_pred_contact[:6]])\n",
    "lbd_n=lbdin_pred_contact[8:]\n",
    "cmin=torch.cat([lbd_p,lbd_n,lbdsy_pred_contact[6:]])\n",
    "cmin=torch.tensor(cmin,dtype=torch.float32).to(device)\n",
    "cmin=cmin.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c258e12f-b7d5-437d-a564-6f3545120fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_255787/4231155913.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y=torch.tensor(y,dtype=torch.long).to(device)\n"
     ]
    }
   ],
   "source": [
    "yp=torch.ones(len(lbd_p))\n",
    "yn=torch.zeros(len(lbd_n))\n",
    "y_=torch.zeros(len(lbdsy_pred_contact[6:]))\n",
    "y=torch.cat([yp,yn,y_])\n",
    "y=torch.tensor(y,dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08766497-5de1-4f53-a386-5a504cb61fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1629.89it/s]\n"
     ]
    }
   ],
   "source": [
    "loss_list=[]\n",
    "accurate_list=[]\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    contactmodel.train()\n",
    "    outputs=contactmodel(cmin)\n",
    "    \n",
    "    loss = criterion(outputs, y)\n",
    "    _, prediction = outputs.max(dim=1)\n",
    "    accurate = (prediction==y).sum().item()\n",
    "        \n",
    "    loss.backward()# calulate loss\n",
    "    optimizer.step()# update gradient\n",
    "    optimizer.zero_grad()# reset gradient\n",
    "        \n",
    "    train_accuracy = accurate / len(outputs)\n",
    "    average_loss = loss / len(outputs)\n",
    "        \n",
    "    loss_list.append(average_loss)\n",
    "    accurate_list.append(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bf8e639-df47-4427-9afa-5b6b52ce257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3e2627afa0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.tensor(accurate_list))\n",
    "plt.plot(torch.tensor(loss_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7067b0e8-f962-4870-b5a6-62a1836f5dbb",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "745832f9-0a4f-4982-93e8-7206d203f5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContactMapRegression(\n",
       "  (regression): Linear(in_features=484, out_features=2, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contactmodel=torch.load('../models/contactmap_filter_planb.pt')\n",
    "contactmodel.eval()\n",
    "contactmodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f70b3fd5-bf42-4e83-9644-8413c06d17bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_255787/2454322881.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  lbdall_pred_contact=torch.tensor(lbdall_pred_contact,dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "index=torch.where(predictions!=0)\n",
    "lbdall_pred_contact=get_pred_contact(lbdin_outputs[index])\n",
    "lbdall_pred_contact=torch.tensor(lbdall_pred_contact,dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46d1c357-b497-49bb-8e35-d5e7d4f0326d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_255787/2803984175.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels=torch.tensor(labels,dtype=torch.long).numpy()\n"
     ]
    }
   ],
   "source": [
    "predictions=contactmodel(lbdall_pred_contact)\n",
    "_,prediction=predictions.max(dim=1)\n",
    "prediction=prediction.cpu()\n",
    "labels=torch.cat([torch.ones(2),torch.zeros(9)])\n",
    "labels=torch.tensor(labels,dtype=torch.long).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ea9d395-9362-4e33-a79b-51e2a44ce91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "[[7 2]\n",
      " [0 2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88         7\n",
      "           1       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.82        11\n",
      "   macro avg       0.89      0.75      0.77        11\n",
      "weighted avg       0.86      0.82      0.80        11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "confusion_mat = confusion_matrix(labels, prediction.cpu())\n",
    "report = classification_report(prediction.cpu(), labels)\n",
    "print(f\"Confusion matrix: \\n{confusion_mat}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "187ea7c6-2710-493a-bdca-aed159b508d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# torch.save(contactmodel, './models/contactmap_filter_planb.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b332d6e-977c-437e-bf36-640bca664453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMP (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
