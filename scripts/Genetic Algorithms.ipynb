{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bca1ef5-ac4e-4584-87ad-931cda947078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import EsmTokenizer, EsmForSequenceClassification, EsmModel, EsmConfig\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f27fe5f-97ba-4726-abe4-0e491f99fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set PyTorch to use GPU or CPU accordingly\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24e76e13-60f1-45f3-b471-96db42f2536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"../models/esm2_650M\"\n",
    "tokenizer = EsmTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87e62a69-5bdb-49fe-8a70-2ff94a38c01e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/esm2_650M were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at ../models/esm2_650M and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): EsmForSequenceClassification(\n",
       "      (esm): EsmModel(\n",
       "        (embeddings): EsmEmbeddings(\n",
       "          (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (position_embeddings): Embedding(1026, 1280, padding_idx=1)\n",
       "        )\n",
       "        (encoder): EsmEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-32): 33 x EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1280, out_features=1280, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.6, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=48, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=48, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(\n",
       "                    in_features=1280, out_features=1280, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.6, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=48, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=48, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (value): Linear(\n",
       "                    in_features=1280, out_features=1280, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.6, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=48, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=48, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): EsmIntermediate(\n",
       "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (contact_head): EsmContactPredictionHead(\n",
       "          (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): EsmClassificationHead(\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out_proj): Linear(in_features=1280, out_features=2, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): EsmClassificationHead(\n",
       "            (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (out_proj): Linear(in_features=1280, out_features=2, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "config = PeftConfig.from_pretrained('../models/esm2_650M_LORA_SEQ_CLS_0.99')\n",
    "model = EsmForSequenceClassification.from_pretrained(\"../models/esm2_650M\", num_labels=2)\n",
    " \n",
    "model = PeftModel.from_pretrained(model, '../models/esm2_650M_LORA_SEQ_CLS_0.99')\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c42b1a4-52d5-4b4d-bcd0-04627534f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasta_dict(fasta_file):\n",
    "    fasta_dict = {}\n",
    "    with open(fasta_file, 'r') as infile:\n",
    "        for line in infile:\n",
    "            if line.startswith(\">\"):\n",
    "                head = line.replace(\"\\n\", \"\").replace(\">\", \"\")\n",
    "                fasta_dict[head] = ''\n",
    "            else:\n",
    "                fasta_dict[head] += line.replace(\"\\n\", \"\")\n",
    "    return fasta_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de5a465-e427-4dfe-9861-26ec05bd477c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Genetic Algorithms\n",
    "<font size=3>We employ property-based model to filter sequences within the positive range as preferred offsprings  \n",
    "<font size=3>It takes about 19 minutes to generate sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3577a38-9388-44a6-ade6-92d864d74ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 20\n"
     ]
    }
   ],
   "source": [
    "test_dict=get_fasta_dict('../database/LBD.fasta')\n",
    "initial_pseqs=[]\n",
    "initial_nseqs=[]\n",
    "for i in test_dict.keys():\n",
    "    if i.split('|')[-1] == 'positive':\n",
    "        initial_pseqs.append(test_dict[i])\n",
    "    else:\n",
    "        initial_nseqs.append(test_dict[i])\n",
    "print(len(initial_pseqs),len(initial_nseqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0851928c-19a5-4532-b7ca-036f57c8c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_pt = tokenizer(initial_pseqs, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=24).to(\"cuda\")\n",
    "initial_nt = tokenizer(initial_nseqs, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=24).to(\"cuda\")\n",
    "p_outputs=model.esm(**initial_pt,output_attentions=True,output_hidden_states=True)\n",
    "n_outputs=model.esm(**initial_nt,output_attentions=True,output_hidden_states=True)\n",
    "initial_pe=p_outputs.last_hidden_state.mean(1)\n",
    "initial_ne=n_outputs.last_hidden_state.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0d5387c-6516-4da3-b231-c34e56e17edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acid = {\n",
    "    4: 'L', \n",
    "    5: 'A', \n",
    "    6: 'G', \n",
    "    7: 'V', \n",
    "    8: 'S', \n",
    "    9: 'E', \n",
    "    10: 'R', \n",
    "    11: 'T', \n",
    "    12: 'I', \n",
    "    13: 'D',\n",
    "    14: 'P', \n",
    "    15: 'K', \n",
    "    16: 'Q', \n",
    "    17: 'N', \n",
    "    18: 'F', \n",
    "    19: 'Y', \n",
    "    20: 'M', \n",
    "    21: 'H', \n",
    "    22: 'W', \n",
    "    23: 'C'}\n",
    "hydrophobicity={\n",
    "    4: 1.700,\n",
    "    5: 0.310,\n",
    "    6: 0.,\n",
    "    7: 1.220,\n",
    "    8: -0.040,\n",
    "    9: -0.640,\n",
    "    10: -1.010,\n",
    "    11: 0.260,\n",
    "    12: 1.800,\n",
    "    13: -0.770,\n",
    "    14: 0.720,\n",
    "    15: -0.990,\n",
    "    16: -0.220,\n",
    "    17: -0.600,\n",
    "    18: 1.790,\n",
    "    19: 0.960,\n",
    "    20: 1.230,\n",
    "    21: 0.130,\n",
    "    22: 2.250,\n",
    "    23: 1.540\n",
    "}\n",
    "dic_new = dict(zip(amino_acid.values(), amino_acid.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "268c685f-ab61-4454-a71d-5b5206657247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 23\n"
     ]
    }
   ],
   "source": [
    "seq_ids=[]\n",
    "for seq,ids in dic_new.items():\n",
    "    seq_ids.append(ids)\n",
    "print(min(seq_ids),max(seq_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6510b046-41a6-4a7c-9869-704a083a27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_func(x):\n",
    "    \"\"\"Use the fine-tuned model to get the index of the physicochemistry properties in changes\n",
    "        First, wrap the input tokens\n",
    "        Then, use the fine-tuned model to predict the physicochemistry properties\n",
    "        Finally, filter the in distribution embeddings and return the index\n",
    "    \"\"\"\n",
    "    new_population=add(x)\n",
    "    #Wrap the input tokens\n",
    "    attention_mask=np.ones((20,24))\n",
    "    attention_mask=attention_mask.astype(int)\n",
    "    new_population=torch.tensor(new_population).to(device)\n",
    "    attention_mask=torch.tensor(attention_mask).to(device)\n",
    "    initial_nt_change={}\n",
    "    initial_nt_change={'input_ids':new_population,'attention_mask':attention_mask}\n",
    "    #Extract physicochemical properties in embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**initial_nt_change)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        predictions=predictions.cpu()\n",
    "    index=np.where(predictions==0)\n",
    "    \n",
    "    return index[0]\n",
    "\n",
    "def add(pop):\n",
    "    \"\"\"Add special tokens and Cys token to data\"\"\"\n",
    "    new_population=list()\n",
    "    for i in pop:\n",
    "        pop=np.insert(i,0,[0,23])\n",
    "        pop=np.insert(pop,len(pop),[23,2])\n",
    "        new_population.append(pop)\n",
    "    new_population=np.array(new_population)\n",
    "    return new_population\n",
    "\n",
    "def crossover(parents, offspring_size):\n",
    "    offspring = np.empty(offspring_size)\n",
    "    # The point at which crossover takes place between two parents. Usually, it is at the center.\n",
    "    crossover_point = np.uint8(offspring_size[1]/2)\n",
    "\n",
    "    for k in range(offspring_size[0]):\n",
    "        # Index of the first parent to mate.\n",
    "        parent1_idx = k%parents.shape[0]\n",
    "        # Index of the second parent to mate.\n",
    "        parent2_idx = (k+1)%parents.shape[0]\n",
    "        # The new offspring will have its first half of its genes taken from the first parent.\n",
    "        offspring[k, 0:crossover_point] = parents[parent1_idx, 0:crossover_point]\n",
    "        # The new offspring will have its second half of its genes taken from the second parent.\n",
    "        offspring[k, crossover_point:] = parents[parent2_idx, crossover_point:]\n",
    "    return offspring\n",
    "\n",
    "def mutation(offspring_crossover, num_mutations=1):\n",
    "    mutations_counter = np.uint8(offspring_crossover.shape[1] / num_mutations)\n",
    "    # Mutation changes a number of genes as defined by the num_mutations argument. The changes are random.\n",
    "    for idx in range(offspring_crossover.shape[0]):\n",
    "        gene_idx = mutations_counter - 1\n",
    "        for mutation_num in range(num_mutations):\n",
    "            # The random value to be added to the gene.\n",
    "            random_value = np.random.randint(min(seq_ids),max(seq_ids),1)\n",
    "            offspring_crossover[idx, gene_idx] = random_value\n",
    "            gene_idx = gene_idx + mutations_counter\n",
    "    return offspring_crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a350acbc-c96d-4d2a-af22-ecfdc5696f02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "equation_inputs=initial_pt['input_ids']\n",
    "sol_per_pop = len(initial_nt['input_ids'])\n",
    "num_parents_mating = 8\n",
    "population=initial_nt['input_ids'][:,2:-2].cpu().numpy()\n",
    "num_weights=len(population)\n",
    "pop_size=(sol_per_pop,num_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d73e8-833c-4b44-832c-b1bfd16c1555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015950918197631836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbc4e5ae1734e258c1565066fd5b3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp_in=np.zeros((1,len(population[0])))\n",
    "num_generations = 10000\n",
    "for generation in tqdm(range(num_generations)):\n",
    "    time.sleep(0.05)\n",
    "    # Measuring the fitness of each chromosome in the population.\n",
    "    index = filter_func(population)\n",
    "    population_filter=list()\n",
    "    parents=list()\n",
    "    for i,token in enumerate(population):\n",
    "        if i in index:\n",
    "            population_filter.append(token)\n",
    "        else:\n",
    "            parents.append(token)\n",
    "    \n",
    "    population_filter=np.array(population_filter)\n",
    "    parents=np.array(parents)\n",
    "    if population_filter.size > 0:\n",
    "        for row in population_filter:\n",
    "            if not np.any(np.all(pp_in == row, axis=0)):\n",
    "                pp_in = np.vstack((pp_in, row))\n",
    "    \n",
    "    # Generating next generation using crossover.\n",
    "    if population_filter.size > 0:\n",
    "        offspring_crossover = crossover(population_filter,\n",
    "                                       offspring_size=(pop_size[0]-parents.shape[0], num_weights))\n",
    "    else:\n",
    "        offspring_crossover = crossover(parents,\n",
    "                                       offspring_size=(pop_size[0]-parents.shape[0], num_weights))\n",
    "\n",
    "    # Adding some variations to the offspring using mutation.\n",
    "    offspring_mutation = mutation(offspring_crossover, num_mutations=3)\n",
    "\n",
    "    # Creating the new population based on the parents and offspring.\n",
    "    population[0:parents.shape[0], :] = parents\n",
    "    population[parents.shape[0]:, :] = offspring_mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f28c9-23cb-4888-bf45-b8ec48ec7816",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_in=pp_in.copy()\n",
    "pp_in=np.delete(pp_in,slice(0,11),axis=0)\n",
    "new_population=list()\n",
    "for i in pp_in:\n",
    "    pp_in=np.insert(i,0,23)\n",
    "    pp_in=np.insert(pp_in,len(pp_in),23)\n",
    "    new_population.append(pp_in)\n",
    "new_population=np.array(new_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfaf222-a55f-4670-b93d-d99aa26aabc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# f1=open('./ouputs/LBD_mutation.fasta','w')\n",
    "# for i,token in enumerate(new_population):\n",
    "#     token=token.astype(int)\n",
    "#     seq=''\n",
    "#     for j in token:\n",
    "#         seq+=''.join(amino_acid[j])\n",
    "#     f1.write('>'+str(i)+'\\n'+seq+'\\n')\n",
    "# f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e7d34-d5b1-45b5-99a2-31ef8fb8b5b6",
   "metadata": {},
   "source": [
    "## filter the mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e39df83-f70e-4ef6-bb35-e6b133e6e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional, Dict, NamedTuple, Union, Callable\n",
    "import esm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b337b1eb-117b-44b3-bc78-fa432cf950ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrize(x):\n",
    "    \"Make layer symmetric in final two dimensions, used for contact prediction.\"\n",
    "    return x + x.transpose(-1, -2)\n",
    "\n",
    "def apc(x):\n",
    "    \"Perform average product correct, used for contact prediction.\"\n",
    "    a1 = x.sum(-1, keepdims=True)\n",
    "    a2 = x.sum(-2, keepdims=True)\n",
    "    a12 = x.sum((-1, -2), keepdims=True)\n",
    "\n",
    "    avg = a1 * a2\n",
    "    avg.div_(a12)  # in-place to reduce memory\n",
    "    normalized = x - avg\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1858a7-3717-4fec-94e8-3532da4337db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from https://github.com/facebookresearch/esm/blob/main/esm/modules.py\n",
    "class AttentionLogisticRegression(nn.Module):\n",
    "    \"\"\"Performs symmetrization, apc, and computes a logistic regression on the output features\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features:int,\n",
    "        prepend_bos: bool,\n",
    "        append_eos: bool,\n",
    "        bias=True,\n",
    "        eos_idx: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features=in_features\n",
    "        self.prepend_bos = prepend_bos\n",
    "        self.append_eos = append_eos\n",
    "        if append_eos and eos_idx is None:\n",
    "            raise ValueError(\"Using an alphabet with eos token, but no eos token was passed in.\")\n",
    "        self.eos_idx = eos_idx\n",
    "        self.regression = nn.Linear(in_features, 1, bias)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, attentions):\n",
    "        # remove eos token attentions\n",
    "        if self.append_eos:\n",
    "            eos_mask = tokens.ne(self.eos_idx).to(attentions)\n",
    "            eos_mask = eos_mask.unsqueeze(1) * eos_mask.unsqueeze(2)\n",
    "            attentions = attentions * eos_mask[:, None, None, :, :]\n",
    "            attentions = attentions[..., :-1, :-1]\n",
    "        # remove cls token attentions\n",
    "        if self.prepend_bos:\n",
    "            attentions = attentions[..., 1:, 1:]\n",
    "        batch_size, layers, heads, seqlen, _ = attentions.size()\n",
    "        attentions = attentions.view(batch_size, layers * heads, seqlen, seqlen)\n",
    "        attentions = attentions.to(self.regression.weight.device)  # attentions always float32, may need to convert to float16\n",
    "        attentions= apc(symmetrize(attentions))\n",
    "        attentions = attentions.permute(0, 2, 3, 1)\n",
    "        \n",
    "        return self.activation(self.regression(attentions).squeeze(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23365103-6bbb-454d-a7e1-7883622e1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel=torch.load('../models/contact-based model.pt')\n",
    "mymodel.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6986441f-2b17-4c38-a22a-60220434f0fd",
   "metadata": {},
   "source": [
    "### Get bceloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66346a9f-3d91-4037-9326-cbc58b2ba836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_contact(attention):\n",
    "    attention=torch.where(attention < 0.9, torch.tensor(0), torch.tensor(1))\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e1a0a-655e-48ce-be21-4b85a1ae8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bceloss(test_output):\n",
    "    loss_list=[]\n",
    "    for number,test in enumerate(test_output):\n",
    "        new_pred=get_pred_contact(test)\n",
    "        loss=criterion(torch.tensor(lbdb,dtype=torch.float32),torch.tensor(new_pred,dtype=torch.float32))\n",
    "        loss_list.append(loss)\n",
    "    \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2006b8-4f9d-477c-bb4a-62483cbc67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d08f1c8-347f-4a5f-8458-fbac115d3cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbdin_dict=get_fasta_dict('../database/LBD.fasta')\n",
    "lbdin_seqs=[]\n",
    "for header,seq in lbdin_dict.items():\n",
    "    lbdin_seqs.append(seq)\n",
    "\n",
    "lbdin_inputs = tokenizer(lbdin_seqs, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=24).to(device)\n",
    "lbdin_outputs=model.esm(**lbdin_inputs,output_attentions=True,output_hidden_states=True)\n",
    "lbdin_attention=torch.stack(lbdin_outputs.attentions,1)\n",
    "lbdin_outputs=mymodel(lbdin_inputs['input_ids'],lbdin_attention).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0dc40a-5436-4ef7-bac6-e40c3c0f969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbdsy_dict=get_fasta_dict('../database/LBD_test.fasta')\n",
    "lbdsy_seqs=[]\n",
    "for header,seq in lbdsy_dict.items():\n",
    "    lbdsy_seqs.append(seq)\n",
    "\n",
    "lbdsy_inputs = tokenizer(lbdsy_seqs, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=24).to(device)\n",
    "lbdsy_outputs=model.esm(**lbdsy_inputs,output_attentions=True,output_hidden_states=True)\n",
    "lbdsy_attention=torch.stack(lbdsy_outputs.attentions,1)\n",
    "lbdsy_outputs=mymodel(lbdsy_inputs['input_ids'],lbdsy_attention).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae0f46-c10d-4b8e-9ec5-dba8ed17dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbdb=lbdsy_outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85452c7-0ac3-4b12-a045-e9e988fa2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_dict=get_fasta_dict('../outputs/LBD_mutation.fasta')\n",
    "seqs=[]\n",
    "for header,seq in mutation_dict.items():\n",
    "    seqs.append(seq)\n",
    "inputs = tokenizer(seqs, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=24).to(device)\n",
    "outputs=model.esm(**inputs,output_attentions=True,output_hidden_states=True)\n",
    "mutation_attention=torch.stack(outputs.attentions,1)\n",
    "mutation_outputs=mymodel(inputs['input_ids'],mutation_attention).cpu()\n",
    "mutation_contact=get_pred_contact(mutation_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d04ad0-54ff-4cb2-ad07-94d693f4ef20",
   "metadata": {},
   "source": [
    "#### Plan A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988dd881-9533-4f1e-9dac-4364eb9de14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pred_contact(test_output):\n",
    "    index_list=[]\n",
    "    for index,attention in enumerate(test_output):\n",
    "        pred_contact=get_pred_contact(attention)\n",
    "        if pred_contact[16][18] ==1 & pred_contact[13][16] ==1:\n",
    "            index_list.append(index)\n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11725be1-6a70-4314-8c45-9a25c17bf093",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_index=filter_pred_contact(mutation_outputs)\n",
    "pp_filter=mutation_outputs[mutation_index]\n",
    "loss_list=get_bceloss(pp_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f9786e-7f6e-438b-bec1-77bfd73c374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bceloss=torch.tensor(loss_list)\n",
    "values,indices=torch.topk(bceloss,k=5,largest=False)\n",
    "for i in indices:\n",
    "    seq=mutation_dict[list(mutation_dict.keys())[mutation_index[i]]]\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ffef62-88bd-4c90-8a4e-6e0bd40d618d",
   "metadata": {},
   "source": [
    "#### Plan B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0245a8-3488-4902-9cc3-040177e890aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContactMapRegression(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features:int,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features=in_features\n",
    "        self.regression = nn.Linear(in_features, 2, bias)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,contact_map):\n",
    "        contact_map=contact_map.reshape((contact_map.shape[0],contact_map.shape[1]*contact_map.shape[1]))\n",
    "        outputs=self.activation(self.regression(contact_map))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb61b4b-3786-4348-939c-1371ababf54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contactmodel=torch.load('../models/contactmap_filter_planb.pt')\n",
    "contactmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa2c30-31fd-40d5-8a1b-00111623495e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pe_outputs=torch.cat([lbdin_outputs[:8],lbdsy_outputs[:6]],dim=0)\n",
    "pe_contact=get_pred_contact(pe_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046c984-0d95-485b-8c52-b0c886e11a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bceloss(seq):\n",
    "    lbd2=get_pred_contact(seq)\n",
    "    \n",
    "    loss_list=[]\n",
    "    for lbd in lbd2:\n",
    "        loss=0\n",
    "        for i in pe_contact:\n",
    "            loss+=criterion(torch.tensor(i,dtype=torch.float32),torch.tensor(lbd,dtype=torch.float32)).item()\n",
    "        average_loss = loss / len(pe_contact)\n",
    "        loss_list.append(average_loss)\n",
    "    \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ae512-092b-4454-80ad-9a9cf1df7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_pred_contacts=get_pred_contact(mutation_outputs)\n",
    "mutation_pred_contacts=torch.tensor(mutation_pred_contacts,dtype=torch.float32).to(device)\n",
    "contact_outputs = contactmodel(mutation_pred_contacts)\n",
    "_,contact_prediction=contact_outputs.max(dim=1)\n",
    "index=np.where(contact_prediction.cpu()==1)\n",
    "pp_filter=mutation_outputs[index[0]]\n",
    "loss_list=test_bceloss(pp_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56899654-817c-49cd-a326-4a1cfc28f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bceloss=torch.tensor(loss_list)\n",
    "values,indices=torch.topk(bceloss,k=5,largest=False)\n",
    "for i in indices:\n",
    "    seq=mutation_dict[list(mutation_dict.keys())[index[0][i]]]\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f65dc-0eee-4fbf-be8b-ee022348f833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMP (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
